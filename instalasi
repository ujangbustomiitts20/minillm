openai@openai:~/llama$ git clone https://github.com/ggml-org/llama.cpp.git
openai@openai:~/llama$ cd llama.cpp
openai@openai:~/llama/llama.cpp$ sudo apt install cmake build-essential libstdc++-12-dev
openai@openai:~/llama/llama.cpp$ mkdir build
openai@openai:~/llama/llama.cpp$ cd build
openai@openai:~/llama/llama.cpp/build$ sudo apt update
openai@openai:~/llama/llama.cpp/build$ sudo apt install libcurl4-openssl-dev
openai@openai:~/llama/llama.cpp/build$ cmake ..
-- Warning: ccache not found - consider installing it for faster compilation or disable this warning with GGML_CCACHE=OFF
-- CMAKE_SYSTEM_PROCESSOR: x86_64
-- GGML_SYSTEM_ARCH: x86
-- Including CPU backend
-- x86 detected
-- Adding CPU backend variant ggml-cpu: -march=native 
-- Found CURL: /usr/lib/x86_64-linux-gnu/libcurl.so (found version "8.5.0")  
-- Configuring done (0.5s)
-- Generating done (0.2s)
-- Build files have been written to: /home/openai/llama/llama.cpp/build
openai@openai:~/llama/llama.cpp/build$ cmake --build . --config Release
[  0%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  0%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml.cpp.o
[  1%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  1%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[100%] Linking CXX executable ../../bin/llama-cvector-generator
[100%] Built target llama-cvector-generator
[100%] Building CXX object tools/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[100%] Linking CXX executable ../../bin/llama-export-lora

openai@openai:~/llama/llama.cpp/build$ cmake --build . --target llama-server -j
[  0%] Built target build_info
[  9%] Built target ggml-base
[ 32%] Built target ggml-cpu
[ 35%] Built target ggml
[ 70%] Built target llama
[ 77%] Built target mtmd
[ 96%] Built target common
[100%] Built target llama-server

menjalankan llama server
openai@openai:~/llama/llama.cpp/build$ ./bin/llama-server -hf ggml-org/SmolVLM-500M-Instruct-GGUF

testing di comman
openai@openai:~/llama/llama.cpp/build$ ./bin/llama-cli   --hf-repo ggml-org/gemma-3-1b-it-GGUF   --hf-file gemma-3-1b-it-Q4_K_M.gguf   -p "Apa itu kecerdasan buatan?"   -n 200

Tambahkan ke PATH agar bisa di akses di mana saja
openai@openai:~/llama/llama.cpp/build$ export PATH=$PATH:/home/openai/llama/llama.cpp/build/bin

# simple usage with CLI
openai@openai:~/llama$ llama-mtmd-cli -hf ggml-org/gemma-3-4b-it-GGUF

# simple usage with server
openai@openai:~/llama$ llama-server -hf ggml-org/gemma-3-4b-it-GGUF

# using local file
openai@openai:~/llama$ llama-server -m gemma-3-4b-it-Q4_K_M.gguf --mmproj mmproj-gemma-3-4b-it-Q4_K_M.gguf  --host 0.0.0.0 --port 8071

# no GPU offload
openai@openai:~/llama$ llama-server -hf ggml-org/gemma-3-4b-it-GGUF --no-mmproj-offload  --host 0.0.0.0 --port 8071

# Use a local model file
openai@openai:~/llama$ llama-cli -m my_model.gguf

# Or download and run a model directly from Hugging Face
openai@openai:~/llama$ llama-cli -hf ggml-org/gemma-3-1b-it-GGUF


# Launch OpenAI-compatible API server
openai@openai:~/llama$ llama-server -hf ggml-org/gemma-3-1b-it-GGUF
